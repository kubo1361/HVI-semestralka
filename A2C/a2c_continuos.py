import numpy as np
import torch
import torch.nn.functional as F
from tqdm.notebook import tqdm
from A2C.a2c_discrete import DiscreteA2C
import math


class ContinuousA2C(DiscreteA2C):
    def __init__(self, model_name, model, gamma=0.99, lr=0.001, beta_entropy=0.01, critic_loss_coef=0.5, id=0):
        super().__init__(model_name, model, gamma, lr, beta_entropy, critic_loss_coef, id)

    def _reset_iter_variables(self, steps, workers):
        len_workers = len(workers)

        crit_vals = torch.zeros(
            [steps, len_workers, 1]).type(self.tensor).to(self.device)

        # Unlike discrete A2C, here we can have multiple continuous actions
        # We change actor_log_probs and entropies accordingly
        actor_log_probs = torch.zeros(
            [steps, len_workers, self.model.actions_count]).type(self.tensor).to(self.device)
        entropies = torch.zeros([steps, len_workers, self.model.actions_count]).type(
            self.tensor).to(self.device)

        rewards = torch.zeros([steps, len_workers, 1]).type(
            self.tensor).to(self.device)
        not_terminated = torch.ones(
            [steps, len_workers, 1]).type(self.tensor).to(self.device)
        return crit_vals, actor_log_probs, entropies, rewards, not_terminated

    def _extract_action(self, x):
        # Continuous environment requires an array of values instead of a single value like discrete env
        return x.detach().cpu().numpy()

    def _agent_step(self, observations):
        # Continuous actions
        # forward pass - mean and variance and first critic values
        step_actor_mean, step_actor_variance, step_critic = self.model(
            observations)

        # extract actions -> Action is chosen from Normal distribution using mean and variance generated by our network
        sigma = torch.sqrt(step_actor_variance).data
        step_actions = torch.normal(step_actor_mean, sigma)
        step_actions = torch.clip(step_actions, -1, 1)

        # step logs prob policy calculation -> Different from discrete A2C
        p1 = - ((step_actor_mean - step_actions) ** 2) / \
            (2*step_actor_variance.clamp(min=1e-3))
        p2 = - torch.log(torch.sqrt(2 * math.pi * step_actor_variance))
        step_log_probs_policy = p1 + p2

        # Entropy calculation -> Also different from discrete A2C
        step_entropies = (-(torch.log(2*math.pi *
                          step_actor_variance) + 1)/2)

        return step_actions, step_log_probs_policy, step_critic, step_entropies

    def _getCriticValues(self, observations):
        # Required because discrete model returns 2 values and continuous model returns 3 value, using this method we dont have to rewrite practicly identical _compute_advantage_iter method
        with torch.no_grad():
            _, _, critic_values = self.model(observations)
        return critic_values.detach()

    def act(self, observation):
        # As said previously, continuous actions are chosen differently then discrete
        self.model.eval()
        obs = torch.from_numpy(observation).float()
        step_actor_mean, step_actor_variance, _ = self.model(
            obs)
        sigma = torch.sqrt(step_actor_variance).data
        step_actions = torch.normal(step_actor_mean, sigma)
        step_actions = torch.clip(step_actions, -1, 1)
        return self._extract_action(step_actions)
